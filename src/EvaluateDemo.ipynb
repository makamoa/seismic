{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "from skimage import io, transform\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "sys.path.append(\"../src/models/\")\n",
    "from swin import BaseSwinUnet\n",
    "from restormer import BaseRestormer\n",
    "from bunet import BaseUnet\n",
    "from noiseadding import build_noise_transforms, CombinedTransforms\n",
    "from data import get_train_val_dataset, get_dataset, get_train_val_dataset\n",
    "from metrics import ConfusionMatrix, RMSE\n",
    "import torchvision\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model, problem):\n",
    "    if model == 'restormer':\n",
    "        if problem == 'deraining':\n",
    "            model = BaseRestormer(inp_channels=3, out_channels=3, dim=24)\n",
    "        elif problem == 'denoise':\n",
    "            model = BaseRestormer(inp_channels=1, out_channels=1, dim=24, activation='tanh')\n",
    "        elif problem == 'firstbreak':\n",
    "            model = BaseRestormer(inp_channels=1, out_channels=2, dim=24)\n",
    "        else:\n",
    "            raise ValueError('Undefined problem!')\n",
    "    elif model == 'swin':\n",
    "        if problem == 'deraining':\n",
    "            model = BaseSwinUnet(in_chans=3, num_classes=3, embed_dim=48)\n",
    "        elif problem == 'denoise':\n",
    "            model = BaseSwinUnet(in_chans=1, num_classes=1, embed_dim=48, activation='tanh')\n",
    "        elif problem == 'firstbreak':\n",
    "            model = BaseSwinUnet(in_chans=1, num_classes=2, embed_dim=48)\n",
    "        else:\n",
    "            raise ValueError('Undefined problem!')\n",
    "    elif model == 'unet':\n",
    "        if problem == 'deraining':\n",
    "            model = BaseUnet(in_channels=3, out_channels=3)\n",
    "        elif problem == 'denoise':\n",
    "            model = BaseUnet(in_channels=1, out_channels=1, activation='tanh')\n",
    "        elif problem == 'firstbreak':\n",
    "            model = BaseUnet(in_channels=1, out_channels=2)\n",
    "        else:\n",
    "            raise ValueError('Undefined problem!')\n",
    "    else:\n",
    "        raise ValueError('Undefined model!')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type='unet'\n",
    "problem='firstbreak'\n",
    "if problem == 'firstbreak':\n",
    "    metrics = ConfusionMatrix(2, train_loader.dataset.dataset.class_names)\n",
    "else:\n",
    "    metrics = RMSE()\n",
    "noise_type = -1\n",
    "noise_scale = 0.0\n",
    "batch_size=8\n",
    "workers=4\n",
    "METADATA = '../metadata/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/makam0a/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaseUnet(\n",
       "  (unet): UNet(\n",
       "    (encoder1): Sequential(\n",
       "      (enc1conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (enc1norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (enc1relu1): ReLU(inplace=True)\n",
       "      (enc1conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (enc1norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (enc1relu2): ReLU(inplace=True)\n",
       "    )\n",
       "    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (encoder2): Sequential(\n",
       "      (enc2conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (enc2norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (enc2relu1): ReLU(inplace=True)\n",
       "      (enc2conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (enc2norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (enc2relu2): ReLU(inplace=True)\n",
       "    )\n",
       "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (encoder3): Sequential(\n",
       "      (enc3conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (enc3norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (enc3relu1): ReLU(inplace=True)\n",
       "      (enc3conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (enc3norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (enc3relu2): ReLU(inplace=True)\n",
       "    )\n",
       "    (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (encoder4): Sequential(\n",
       "      (enc4conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (enc4norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (enc4relu1): ReLU(inplace=True)\n",
       "      (enc4conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (enc4norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (enc4relu2): ReLU(inplace=True)\n",
       "    )\n",
       "    (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (bottleneck): Sequential(\n",
       "      (bottleneckconv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bottlenecknorm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bottleneckrelu1): ReLU(inplace=True)\n",
       "      (bottleneckconv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bottlenecknorm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bottleneckrelu2): ReLU(inplace=True)\n",
       "    )\n",
       "    (upconv4): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (decoder4): Sequential(\n",
       "      (dec4conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (dec4norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (dec4relu1): ReLU(inplace=True)\n",
       "      (dec4conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (dec4norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (dec4relu2): ReLU(inplace=True)\n",
       "    )\n",
       "    (upconv3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (decoder3): Sequential(\n",
       "      (dec3conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (dec3norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (dec3relu1): ReLU(inplace=True)\n",
       "      (dec3conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (dec3norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (dec3relu2): ReLU(inplace=True)\n",
       "    )\n",
       "    (upconv2): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (decoder2): Sequential(\n",
       "      (dec2conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (dec2norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (dec2relu1): ReLU(inplace=True)\n",
       "      (dec2conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (dec2norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (dec2relu2): ReLU(inplace=True)\n",
       "    )\n",
       "    (upconv1): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (decoder1): Sequential(\n",
       "      (dec1conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (dec1norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (dec1relu1): ReLU(inplace=True)\n",
       "      (dec1conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (dec1norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (dec1relu2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (activation): Identity()\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model(model_type, problem)\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "weight_file = 'unet_firstbreak_noisetype_-1_noisescale_0.0'\n",
    "save_path = os.path.join(METADATA, weight_file + '.pkl')\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, metrics):\n",
    "    metrics.reset()\n",
    "    for i, (sample) in enumerate(loader):\n",
    "        x, y = sample['input'].float(), sample['target'].numpy()\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x)\n",
    "            if problem == 'firstbreak':\n",
    "                y_pred = torch.argmax(y_pred, dim=1) # get the most likely prediction\n",
    "        metrics.add_batch(y, y_pred.detach().cpu().numpy())\n",
    "        print('_', end='')\n",
    "    return metrics.get()\n",
    "\n",
    "def evaluate_robustness(model, metrics):\n",
    "    robustness = np.zeros([4,4])\n",
    "    for i, noise_type in enumerate(range(4)):\n",
    "        for j, noise_scale in enumerate([0.25,0.5,1.0,2.0]):\n",
    "            noise_transforms = build_noise_transforms(noise_type=noise_type, scale=noise_scale)\n",
    "            denoise_dataset = get_dataset('firstbreak', noise_transforms=noise_transforms)\n",
    "            _, val_dataset = get_train_val_dataset(denoise_dataset)\n",
    "            valid_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=workers)\n",
    "            robustness[i, j] = evaluate(model, valid_loader, metrics)\n",
    "            print(noise_type, noise_scale, robustness[i, j])\n",
    "    return robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________________"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9996205034969137"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_transforms = build_noise_transforms(noise_type=-1, scale=0)\n",
    "denoise_dataset = get_dataset('firstbreak', noise_transforms=noise_transforms)\n",
    "train_dataset, val_dataset = get_train_val_dataset(denoise_dataset)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "valid_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=workers)\n",
    "\n",
    "evaluate(model, valid_loader, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________________0 0.25 0.3578163323416848\n",
      "___________________________0 0.5 0.3560139472063022\n",
      "___________________________0 1.0 0.35224260776552563\n",
      "___________________________0 2.0 0.3520435422671147\n",
      "___________________________1 0.25 0.3585580292324308\n",
      "___________________________1 0.5 0.3548119071079891\n",
      "___________________________1 1.0 0.35676015078392054\n",
      "___________________________1 2.0 0.35255833265528175\n",
      "___________________________2 0.25 0.3555927960516808\n",
      "___________________________2 0.5 0.35291179466804573\n",
      "___________________________2 1.0 0.3521714130980177\n",
      "___________________________2 2.0 0.35760073529909403\n",
      "____"
     ]
    }
   ],
   "source": [
    "robustness = evaluate_robustness(model, metrics)\n",
    "np.save(os.path.join(METADATA, 'robustness_' + weight_file + '.npy'),np.array(robustness))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load(os.path.join(METADATA, 'robustness_' + weight_file + '.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_cm = pd.DataFrame(robustness, index = [i for i in ['gauss+color', '+linear', '+fft', '+hyperbolic']],\n",
    "                  columns = [i for i in [1,2,4,8]])\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "plt.xlabel('noise to signal')\n",
    "plt.ylabel('noise types')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-new",
   "language": "python",
   "name": "transformer-new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
